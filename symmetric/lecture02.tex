\chapter{Моделі джерел відкритого тексту. Ентропія на символ джерела}

При шифруванні текст перетворюється таким чином, щоб зробити його зміст
незрозумілим для того, хто не знає секретного ключа. Для побудови математичної
теорії криптографічних систем шифрування потрібно насамперед дати математичний
опис (або математичну модель) тексту та перетворень, які відбуваються з ним під
час шифрування.

\begin{definition}[Алфавіт]
    Надалі вважатимемо, що алфавіт є скінченним.
    Позначимо алфавіт як $Z_m = \left\{z_1, \dots , z_m \right\}$, де
    $z_1, \dots , z_{m}$ --- букви (символи) алфавіту.
    Елементами алфавіту можуть бути власне букви; букви та цифри;
    букви, цифри та знаки пунктуації, взагалі скінченний набір
    будь-яких символів, наприклад, танцюючі чоловічки.
    Як правило, ми будемо розглядати український,
    російський чи латинський алфавіт (малі букви) зі знаком пропуску,
    що вважається буквою, або без нього, або ж двійковий алфавіт,
    що складається з двох символів: $0$ та $1$. 
\end{definition}
\begin{definition}[Текст]
    Під текстом будемо розуміти послідовність букв деякого алфавіту.
\end{definition}

\begin{definition}[Відкритий текст] Відкритий текст (ВТ) --- це текст,
    що підлягає шифруванню.
\end{definition}
\begin{definition}[Шифрований текст]Шифрований текст (ШТ) --- це текст,
    що утворюється в результаті шифрування.
\end{definition}

Відкритий та шифрований тексти можуть бути записані як у одному й тому ж,
так і у різних алфавітах (більш докладно поняття ВТ та ШТ
розглядаються у лекції 3).

\begin{definition}[n-грама]
n-грамою називається послідовність  $n$ символів тексту, що
стоять підряд. При  ${n}$=2 це біграма, при  ${n}$=3 --- триграма.
\end{definition}

Будь-який текст має певну статистичну структуру. Для опису цієї структури
використовуються різноманітні ймовірнісні моделі мови. 

\begin{definition}[Джерело відкритого тексту]
    Джерело відкритого тексту генерує послідовність символів алфавіту 
    $x_1, x_2, \dots, x_n, \dots$ випадковим чином.
    Джерело визначається алфавітом та ймовірностями появи $n$-грам: 
    $\probability{x_{i+1}=z_1, x_{i+2}=z_2, \dots, x_{i+n}=z_n}$
    для будь-яких цілих  $n \ge 1$, $i \geq 0$
    (тут $x_1, x_2, \dots, x_n$ --- випадкові величини,
    а $z_1, \dots, z_n$ --- букви алфавіту), які мають задовольняти умовам: 

    \begin{enumerate}
        \item Вихід джерела ВТ є випадковим процесом
            з дискретним часом та множиною станів  $Z_m$
        $$\sum_{z_1, z_2, \dots, z_n \in Z_m}
            \probability{x_{i+1}=z_1, \dots, x_{i+n}=z_n}=1$$
        \item Умова узгодженості
            скінченновимірних розподілів виходу джерела ВТ:
            Для будь-якого цілого  ${s\ge 1}$
        \begin{align*}
            \sum_{z_1, z_2, \dots, z_n \in Z_m}
                \probability{x_{i+1}=z_1, \dots, x_{i+n}=z_n, \dots, x_{i+n+s}
                    =z_{n+s}} = \\
                = \probability{x_{i+1}=z_1, \dots, x_{i+n}=z_n}
        \end{align*}
    \end{enumerate}
\end{definition}

\begin{definition}[Стаціонарне джерело відкритого тексту]
    Джерело називають стаціонарним, якщо для будь-яких цілих  $n\ge 1,
    1\le i_1<\dots<i_n,j\ge 0$
    і будь-якого набору букв алфавіту  $z_1, \dots,z_n$ виконується  рівність:
    $$\probability{x_{i_1+j}=z_1, x_{i_2+j}=z_2, \dots, x_{i_n+j}=z_n}
        = \probability{x_{i_1}=z_1, x_{i_2}=z_2, \dots, x_{i_n}=z_n}$$
\end{definition}

У подальшому будемо розглядати лише стаціонарні джерела, тобто такі, у яких
немає залежності від зсуву $j$.
Для стаціонарних джерел достатньо задати ймовірності
$\probability{x_1=z_1,\dots,x_n=z_n}$ для ${n\ge 1}$.

В залежності від властивостей сумісних розподілів 
$\probability{x_1=z_1, \dots, x_n=z_n}$, $n\ge 1$
можна побудувати різні моделі джерела ВТ.

Найбільш уживаними є описані нижче чотири моделі, з яких кожна наступна все
більш адекватно відображає структуру мови. Перша з них є простою й менш за всі
враховує реальні статистичні властивості мови. Назвемо її моделлю М0.
\begin{definition}[Модель M0]
    У цій моделі джерело у кожен момент часу генерує символи
    із  $Z_m$ незалежно та рівноімовірно:
    $$\probability{x_i=z}=\frac{1}{m}, z \in Z_m, i = 1, 2, \dots$$

    Всі  $n$-грами в моделі М0 є рівноймовірними
    для  будь-яких цілих  ${n\ge 1}$ та
    $z_1,\dots,z_n \in Z_m$
    $$\probability{x_1=z_1,\dots,x_n}=z_{{n}}=\frac{1}{m^n}$$

    Модель М0 має допоміжний характер,
    бо вона не враховує навіть найпростіших властивостей мови.
\end{definition}

Наступна модель ВТ враховує частоти, з якими окремі букви зустрічаються у мові.

\begin{definition}[Модель M1]
    Символи тексту  $x_1, x_2, \dots, x_n, \dots$ є
    незалежними, але вони генеруються із різними ймовірностями
    $$\probability{x_i=z}=\pdf{z}, z \in Z_m, i=1, 2, \dots$$

    Розподіл ймовірностей  $\pdf{z}$ відповідає частотам появи букв  $z \in Z_m$
    у мові. У цій моделі ймовірність появи n-грами має вигляд:

    $$\probability{x_1=z_1, x_2=z_2, \dots, x_n=z_n}
        = \prod_{i=1}^n \pdf{z_i}$$

    Зазначимо, що ймовірності букв у природних мовах значно відрізняються.
    Наприклад,
    найбільшу частоту в українській та російській мові має буква “о”,
    в англійській --- буква ``e''.
    В російській мові ``о'' зустрічається майже в 50 раз частіше букви ``ф'',
    що має найменшу частоту.\footnote{За умови,
    що твердий знак ототожнюється з м’яким.}
\end{definition}

Наступна модель враховує залежність в мові між двома буквами, що стоять поряд.

\begin{definition}[Модель M2]
    Джерело генерує біграми $x_1 x_2, x_3 x_4, x_5 x_6, \dots$ і
    т.д. незалежно одну від одної.
    Тобто на множині всіх біграм заданий розподіл імовірностей
    $\pdf{z_i, z_j}, i, j = 1 , \dots ,m$, і кожна
    нова біграма джерела генерується незалежно від інших.
\end{definition}

Більш складні залежності мови враховуються за допомогою марковської моделі.

\begin{definition}[Однорідний ланцюг Маркова]
    Однорідний ланцюг Маркова --- це послідовність випадкових величин
    $\left\{ x_i \right\}, i=1,2,\dots$, що 
    приймають значення у дискретній множині $Z$, така,
    що для будь-якого $n \geq 2$ 

    \begin{align*}
        \probability{x_n=z_n \mid x_1=z_1, x_2=z_2, \dots, x_{n-1}=z_{n-1}} = \\
            = \probability{x_n=z_n \mid x_{n-1}=z_{n-1}}
            = p_{z_n z_{n-1}}
    \end{align*}

    Для неоднорідного маркoвського ланцюга імовірності переходу 
    $p_{z_n z_{n-1}}$ залежали б від n, а в нашому випадку вони залежать
    лише від станів $z_{n-1}, z_n$.

\end{definition}

\begin{definition}[Модель M3]
    У цій моделі джерела ВТ послідовність $x_1, x_2, \dots$
    утворює однорідний ланцюг Маркова.
    Для завдання такого маркoвського ланцюга достатньо задати
    розподіл початкових станів  $p_0\left( i \right), i \in Z_m$
    та перехідні ймовірності
    $p_{ij}=\probability{x_{n+1}=j \mid x_n=i}, i, j\in Z_m$,
    які в силу однорідності не залежать від  $n$.
    
    При виконанні деяких умов на ланцюг Маркова (які не суперечать  властивостям
    природних мов) існує граничний розподіл
    $$\pi_i=\lim_{n \to \infty} \probability{x_n=i \mid x_1=j}$$

    що не залежить від початкового стану  $j$. Він називається стаціонарним
    розподілом ймовірностей маркoвського ланцюга. Ці ймовірності задовольняють
    наступним рівностям

    $$\begin{cases}
        \sum_{i \in Z} \pi_i = 1 \\
        \sum_{i \in Z} \pi_i p_{ij} = \pi_j, j \in Z_m
    \end{cases}$$

    Ймовірність $n$-грами у моделі M3 у стаціонарному режимі
    можна записати у вигляді
    $$\probability{x_1=z_1, \dots, x_n=z_n}
        = \pi_{z_1} p_{z_1 z_2} p_{z_2 z_3} \dots p_{z_{n-1} z_n}$$
\end{definition}

Якщо існує стаціонарний розподіл  і $p_0\left( i \right) = \pi_i, i \in Z_m$,
то одноріний ланцюг Маркова --- стаціонарний випадковий процес.

\section{Деякі відомості з теорії інформації}

Нехай  $X=\left\{ x_1, x_2, \dots, x_m \right\}$ ---
скінченна множина, на якій заданий розподіл ймовірностей 
$P = \left\{ \pdf{x_1}, p{x_2}, \dots, p{x_m} \right\}$.

\begin{definition}[Повідомлення]
    Елементи  $x_i$ будемо називати повідомленнями
\end{definition}

\begin{definition}[Скінченний ансамбль]
    Пара $\left( X, P \right)$ в теорії інформації
    називається ансамблем (скінченним).
\end{definition}

Часто говорять просто про ансамбль $X$, розуміючи під цим пару
$\left( X, P \right)$.

Інтуїтивно зрозуміло, що малоймовірне повідомлення несе у собі  більше
інформації, ніж більш імовірне. К. Шеннон запропонував для виміру кількості
інформації функцію, яка відповідає цьому інтуїтивному уявленню, і до того ж  є
зручною при обчисленнях.

\begin{definition}[Власна інформацієя повідомлення]
    Власною інформацією повідомлення $x_i$ називається величина
    $$I\left(x_i\right)=-\log{p\left( x_i \right)} \ge 0$$
\end{definition}

Усереднимо власну інформацію за всіма повідомленнями ансамблю

$$H(X)=-\sum_{i=1}^m \pdf{x_i} \cdot \log{\pdf{x_i}}$$

\begin{definition}[Ентропія ансамблю]
Величина  $H\left( X \right)$ називається ентропією ансамблю $X$.
\end{definition}

$H\left( X \right)$ може бути інтерпретована як невизначеність експерименту,
в  якому  з ансамблю $X$ вибирається одне повідомлення, причому
повідомлення $x_i$ може бути вибране з ймовірністю $\pdf{x_i}$, $i=1,2,\dots$

Найчастіше в означенні ентропії
беруть логарифм за основою 2, тоді інформація та ентропія вимірюються в бітах.
Якщо логарифм десятковий ($\lg$), то --- в дитах,
якщо логарифм натуральний ($\ln$), то --- в натах.

Розглянемо декартовий добуток скінченних множин $X$ та $Y$, тобто
множину пар  $\left(x, y\right), x\in X, y\in Y$.
Нехай на множині пар задано розподіл
імовірностей  $\pdf{x, y}$. Тоді говорять, що ансамблі $X$ та $Y$
задані сукупно. Розподіл  $\pdf{x, y}$ індукує розподіли на $X$ та $Y$:

$$\begin{cases}
\pdf{x} &= \sum_{y_j \in Y}\pdf{x,y} \\
\pdf{y} &= \sum_{x_j \in X}\pdf{x,y}
\end{cases}$$

тобто $X$ та $Y$ можна також розглядати і як окремі ансамблі.

\begin{definition}[Сукупна ентропія]
    Сукупною ентропією ансамблів  $X$ та  $Y$ називається величина
    $$H\left( XY \right)=-\sum_{x,y} \pdf{x,y} \cdot \log{\pdf{x,y}}$$
\end{definition}

\begin{definition}[Незалежні ансамблі]
    Сукупно задані ансамблі $X$ та $Y$ називаються
    незалежними, якщо
    $$\forall \left( x,y \right): \pdf{x,y} = \pdf{x} \cdot \pdf{y}$$
\end{definition}

Розглянемо декартовий добуток скінченних множин $X$ та $Y$, тобто
множину пар
$$\left( x,y \right),\; x\in X,y\in Y$$

\begin{definition}[Сукупно задані ансамблі]
Нехай на множині пар задано розподіл імовірностей $\pdf{x,y}$.
Тоді говорять, що ансамблі $X$ та $Y$ задані сукупно.
\end{definition}

Розподіл  $\pdf{x,y}$ індукує розподіли на $X$ та $Y$:
$$\pdf{x}=\sum_{y_j \in Y} \pdf{x,y};\qquad
    \pdf{y}=\sum_{x_j\in X} \pdf{x,y}$$

тобто $X$ та $Y$ можна також розглядати і як окремі ансамблі.

\begin{definition}[Сукупна ентропія]
    Сукупною ентропією ансамблів  $X$ та $Y$ називається величина
    $$H\left( XY \right) = -\sum_{x,y} \pdf{x,y} \cdot \log{\pdf{x,y}}$$
\end{definition}

\begin{definition}[Незалежні ансамблі]
    Сукупно задані ансамблі $X$ та $Y$ називаються незалежними, якщо
    $$\forall \left( x,y \right):\; \pdf{x,y} = \pdf{x} \cdot \pdf{y}$$
\end{definition}

З курсу теорії ймовірностей відомо визначення умовної ймовірності:
$$\pdf{x \mcond y} = \frac{\pdf{x,y}}{\pdf{y}},\; \pdf{y}\neq 0$$

Нехай відомий результат $y$ експерименту $Y$. Тоді можна визначити умовну
ентропію таким чином:
$$H\left(X|y\right) = -\sum_{x\in X} \pdf{x|y} \cdot \log\pdf{x|y}$$

Усереднивши  $H\left( X \mcond y \right)$ за всіма  $y$,
отримаємо умовну ентропію ансамблю $X$ відносно ансамблю $Y$:

$$H\left( X \mcond Y \right)
    = \frac{1}{\left| Y \right|}
        \cdot \sum_{y \in Y} H\left( X \mcond y \right)$$

Властивості ентропії
\begin{enumerate}
    \item  $H\left( X \right) \ge 0$.
    \item  $H(X)=0$ тоді і тільки тоді, коли деяке  $p_i=1$, а всі інші 
        $p_j=0,j\neq i$. ($\log{0}$ не визначений, та оскільки  $p\cdot
        \log{p} \xrightarrow[p \to 0]{} 0$, то за неперервністю
        довизначимо такі доданки в $H\left( X \right)$ як нульові).
    \item $H\left( X \right)$ приймає максимальне значення тоді,
        коли всі $x_i$ є рівноймовірними:
        $p\left( x_i \right) = \frac{1}{m}$. В цьому випадку
        $$H\left( X \right)
            = -\sum_{i=1}^m \frac{1}{m} \cdot \log{\frac{1}{m}}
            =\log{m}$$
    \item Якщо $X$ та $Y$ --- незалежні, то
        $$H\left( XY \right) = H\left( X \right) + H\left( Y \right)$$
    \item $$H\left( X \mcond Y \right)\ge 0$$
    \item Інтуїтивно: сукупна невизначеність експериментів $X$ та $Y$
        дорівнює невизначеності $X$ плюс невизначеність $Y$, що
        залишилася після того, як результат експерименту $X$ став відомим.
        $X$ та $Y$ входять у формулу симетрично
        $$H\left( XY \right)
            = H\left( X \right)+H\left( Y \mcond X \right)
            = H\left( Y \right)+H\left( X \mcond Y \right)$$
    \item Додаткові знання не можуть збільшити невизначеність
        $$H\left( X \right)
            \ge H\left( X \mcond Y \right)
            \ge H\left( X \mcond YZ \right)
            \ge \dots$$
\end{enumerate}

\begin{definition}[Взаємна інформація]
    Взаємною інформацією ансамблів $X$ та $Y$ називається величина 
    $$I\left( X; Y \right)
        = H\left( X \right)-H\left( X \mcond Y \right)
        = H\left( Y \right)-H\left( Y \mcond X \right)$$

    При незалежних ансамблях $X$ та $Y$
    $$I\left( X; Y \right) = 0$$
\end{definition}

Якщо множини $X$ та $Y$ співпадають, то декартовий добуток
$X \times Y$ позначимо як  $X^2$.
Аналогічно, якщо всі $X_i=X$, то
$$X_1 \times X_2 \times \dots \times X_n=X^n$$

\section{Ентропія на символ джерела}
